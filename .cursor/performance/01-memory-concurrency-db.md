# üìñ Go Performance Bible - Part 1

**Memory, Concurrency & Database Optimizations**

**–í–µ—Ä—Å–∏—è:** 2.0 | **–î–∞—Ç–∞:** 01.12.2025 | **Go:** 1.23+

---

# 1Ô∏è‚É£ MEMORY & GC OPTIMIZATIONS

## üî¥ CRITICAL: Memory Pooling (sync.Pool)

```go
var responsePool = sync.Pool{
    New: func() interface{} {
        return &Response{Data: make([]byte, 0, 4096)}
    },
}

func Handler(w http.ResponseWriter, r *http.Request) {
    resp := responsePool.Get().(*Response)
    defer func() {
        resp.Data = resp.Data[:0]
        responsePool.Put(resp)
    }()
}
```

**Gains:** Allocations ‚Üì80-90%, GC pause ‚Üì60-70%, Latency ‚Üì20-30%

---

## üî¥ CRITICAL: Struct Field Alignment

```go
// ‚ùå Bad: 32 bytes
type Player struct {
    IsActive bool   // 1 + 7 padding
    ID       uint64 // 8
    Level    uint8  // 1 + 7 padding
}

// OK Good: 16 bytes (-50%)
type Player struct {
    ID       uint64 // 8
    Level    uint8  // 1
    IsActive bool   // 1
}
```

**Tool:** `fieldalignment ./...`  
**Gains:** Memory ‚Üì30-50%, Cache hits ‚Üë15-20%

---

## üü° HIGH: Preallocation

```go
// ‚ùå Bad: reallocations
items := []Item{}
for i := 0; i < 1000; i++ {
    items = append(items, Item{})
}

// OK Good: preallocate
items := make([]Item, 0, 1000)
```

**Gains:** Allocations ‚Üì90%, CPU ‚Üì15-20%

---

## üü° HIGH: String vs []byte

```go
// Hot path: use []byte
func Process(data []byte) { // Not string!
    bytes.ToUpper(data) // In-place
}
```

**Gains:** Allocations ‚Üì50-70%

---

## üü¢ MEDIUM: Arena Allocator (Go 1.20+)

```go
import "arena"

func ProcessBatch() {
    mem := arena.NewArena()
    defer mem.Free()
    
    for i := 0; i < 10000; i++ {
        item := arena.New[Item](mem)
        process(item)
    }
}
```

**Status:** Experimental  
**Gains:** GC ‚Üì90%, Latency ‚Üì40-60% (batch processing)

---

## üü° HIGH: GC Tuning

```go
import "runtime/debug"

func init() {
    // Low-latency (game servers)
    debug.SetGCPercent(50) // More frequent, shorter pauses
    
    // High-throughput (batch)
    debug.SetGCPercent(200) // Less frequent, higher throughput
    
    // Memory limit (Go 1.19+)
    debug.SetMemoryLimit(2 * 1024 * 1024 * 1024) // 2GB
}
```

**Gains:** GC pause ‚Üì40-60%

---

# 2Ô∏è‚É£ CONCURRENCY OPTIMIZATIONS

## üî¥ CRITICAL: Lock-Free Structures

```go
// ‚ùå Mutex: 50ns/op
type Counter struct {
    mu    sync.Mutex
    value int64
}

// OK Atomic: 5ns/op
type Counter struct {
    value atomic.Int64
}
```

**Gains:** Latency ‚Üì90%, Contention -100%, Throughput ‚Üë300-500%

---

## üî¥ CRITICAL: Worker Pool

```go
type WorkerPool struct {
    semaphore chan struct{}
}

func NewWorkerPool(size int) *WorkerPool {
    return &WorkerPool{semaphore: make(chan struct{}, size)}
}

func (p *WorkerPool) Submit(task func()) {
    p.semaphore <- struct{}{}
    go func() {
        defer func() { <-p.semaphore }()
        task()
    }()
}
```

**Gains:** Memory ‚Üì90%, Scheduler overhead ‚Üì80%

---

## üü° HIGH: SingleFlight Pattern (NEW 2024!)

```go
import "golang.org/x/sync/singleflight"

type Service struct {
    sf singleflight.Group
}

func (s *Service) GetPlayer(ctx context.Context, id string) (*Player, error) {
    // 100 parallel requests ‚Üí 1 execution
    result, err, _ := s.sf.Do(id, func() (interface{}, error) {
        return s.repo.GetPlayer(ctx, id)
    })
    return result.(*Player), err
}
```

**Use cases:** Cache stampede, thundering herd  
**Gains:** DB queries ‚Üì90-95% (burst), Latency ‚Üì80%

---

## üü° HIGH: ErrGroup Pattern (NEW 2024!)

```go
import "golang.org/x/sync/errgroup"

func LoadData(ctx context.Context, id string) error {
    g, ctx := errgroup.WithContext(ctx)
    
    g.Go(func() error { return loadProfile(ctx, id) })
    g.Go(func() error { return loadInventory(ctx, id) })
    g.Go(func() error { return loadStats(ctx, id) })
    
    return g.Wait() // Parallel + error handling
}
```

**Gains:** Latency ‚Üì70% (parallel vs sequential)

---

## üü¢ MEDIUM: Bounded Channels

```go
// OK Always bounded + backpressure
events := make(chan Event, 1000)

select {
case events <- event:
    // OK
case <-time.After(10 * time.Millisecond):
    return ErrOverloaded // Shed load
}
```

**Prevents:** Memory leaks, unbounded growth

---

## üü¢ MEDIUM: Context Timeouts

```go
const (
    DBTimeout    = 50 * time.Millisecond
    CacheTimeout = 10 * time.Millisecond
    ExtTimeout   = 200 * time.Millisecond
)

func (s *Service) Process(ctx context.Context) error {
    ctx, cancel := context.WithTimeout(ctx, DBTimeout)
    defer cancel()
    return s.repo.Query(ctx)
}
```

---

# 3Ô∏è‚É£ DATABASE OPTIMIZATIONS

## üî¥ CRITICAL: Batch Operations

```go
// ‚ùå N+1: 2000ms for 1000 IDs
for _, id := range ids {
    player := db.Get(id) // 1000 queries
}

// OK Batch: 5ms
players := db.GetBatch(ids) // 1 query
```

**Gains:** Queries ‚Üì99%, Latency ‚Üì99.7%, DB load ‚Üì95%

---

## üî¥ CRITICAL: Connection Pooling

```go
db.SetMaxOpenConns(25)
db.SetMaxIdleConns(25)
db.SetConnMaxLifetime(5 * time.Minute)
db.SetConnMaxIdleTime(10 * time.Minute)
```

**Recommendations:**
- Game servers: 25 connections
- API services: 50 connections
- Analytics: 10 connections

---

## üü° HIGH: Prepared Statements

```go
type Repository struct {
    getPlayerStmt *sql.Stmt
}

func NewRepository(db *sql.DB) *Repository {
    stmt, _ := db.Prepare("SELECT * FROM players WHERE id = $1")
    return &Repository{getPlayerStmt: stmt}
}
```

**Gains:** Query planning ‚Üì100%, Latency ‚Üì10-15%

---

## üü¢ MEDIUM: Read Replicas

```go
type Repository struct {
    master  *sql.DB // Writes
    replica *sql.DB // Reads
}

func (r *Repository) GetPlayer(id string) (*Player, error) {
    return r.replica.QueryRow(...) // Read from replica
}
```

**Gains:** Master load ‚Üì70-80%, Read throughput ‚Üë300-500%

---

# 4Ô∏è‚É£ GOROUTINE MANAGEMENT

## üî¥ CRITICAL: Goroutine Leak Detection

```go
import "go.uber.org/goleak"

func TestMain(m *testing.M) {
    goleak.VerifyTestMain(m)
}

func TestNoLeaks(t *testing.T) {
    defer goleak.VerifyNone(t)
    
    svc := NewService()
    svc.Start()
    svc.Stop()
}
```

**Real case:** Uber - +34% speed, memory ‚Üì9.2x after fixing leaks

---

## üü° HIGH: Runtime Tuning

```go
func init() {
    // Dedicated server
    runtime.GOMAXPROCS(runtime.NumCPU() - 1)
    
    // Or let K8s set it (respects CPU limits)
}
```

---

## üü¢ MEDIUM: Defer in Hot Path

```go
// ‚ùå Hot path: defer overhead
func HotPath() {
    mu.Lock()
    defer mu.Unlock() // 50ns overhead
}

// OK Manual unlock
func HotPath() {
    mu.Lock()
    mu.Unlock() // No overhead
}
```

**Nano-optimization:** Only for >100k ops/sec

---

# 5Ô∏è‚É£ ESCAPE ANALYSIS

## üü° HIGH: Keep Data on Stack

```bash
# Check escape analysis
go build -gcflags='-m' ./... 2>&1 | grep "escapes"
```

```go
// OK Stack (fast)
func process() {
    var buffer [1024]byte
    use(buffer[:])
}

// ‚ùå Heap (slow)
func process() *Buffer {
    buffer := make([]byte, 1024)
    return &buffer // Escapes!
}
```

---

# üìä EXPECTED GAINS (Part 1)

## After implementing Part 1:

| Metric | Before | After | Gain |
|--------|--------|-------|------|
| Throughput | 2k/s | 8-10k/s | +400% |
| P99 Latency | 150ms | 30-50ms | -70% |
| Memory | 500MB | 200MB | -60% |
| GC Pause | 15ms | 4-6ms | -70% |
| DB Load | 100% | 20% | -80% |

---

**Next:** Part 2 - Network, Game, Advanced  
**See also:** `.cursor/BACKEND_OPTIMIZATION_CHECKLIST.md`

