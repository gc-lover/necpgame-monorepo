databaseChangeLog:
- changeSet:
    id: documentation-quest-002-harvard-ethical-ai-framework-35210ac4
    author: content-migration-generator
    changes:
    - insert:
        tableName: project.documentation
        columns:
        - column:
            name: id
            value: '-598249985592884'
        - column:
            name: doc_id
            value: quest-boston-2030-2039-harvard-ethical-ai-framework
        - column:
            name: title
            value: Boston 2030-2039 — Этическая основа ИИ Гарварда
        - column:
            name: document_type
            value: canon
        - column:
            name: category
            value: quests
        - column:
            name: status
            value: draft
        - column:
            name: version
            value: 1.0.0
        - column:
            name: last_updated
            value: 2025-12-23 18:11:00+00:00
        - column:
            name: concept_approved
            value: false
        - column:
            name: concept_reviewed_at
            value: ''
        - column:
            name: owners
            value: '[{"role": "content_writer", "contact": "content@necp.game"}]'
        - column:
            name: tags
            value:
            - boston
            - harvard-ethical-ai-framework
            - ai-ethics
            - philosophical-framework
            - ai-governance
        - column:
            name: topics
            value:
            - artificial_intelligence_ethics
            - philosophical_framework_development
            - ai_governance_and_regulation
            - human_ai_relationships
            - moral_reasoning_in_machines
        - column:
            name: related_systems
            value:
            - narrative-service
            - quest-service
            - character-service
            - ai-service
        - column:
            name: related_documents
            value: '[{"id": "lore-harvard-ethical-ai-boston", "relation": "references"}]'
        - column:
            name: source
            value: ''
        - column:
            name: visibility
            value: internal
        - column:
            name: audience
            value:
            - concept
            - narrative
            - liveops
        - column:
            name: risk_level
            value: critical
        - column:
            name: content
            value: '{"metadata": {"id": "quest-boston-2030-2039-harvard-ethical-ai-framework",
              "title": "Boston 2030-2039 — Этическая основа ИИ Гарварда", "document_type":
              "canon", "category": "quests", "status": "draft", "version": "1.0.0",
              "last_updated": "2025-12-23T18:11:00+00:00", "concept_approved": false,
              "concept_reviewed_at": "", "owners": [{"role": "content_writer", "contact":
              "content@necp.game"}], "tags": ["boston", "harvard-ethical-ai-framework",
              "ai-ethics", "philosophical-framework", "ai-governance"], "topics":
              ["artificial_intelligence_ethics", "philosophical_framework_development",
              "ai_governance_and_regulation", "human_ai_relationships", "moral_reasoning_in_machines"],
              "related_systems": ["narrative-service", "quest-service", "character-service",
              "ai-service"], "related_documents": [{"id": "lore-harvard-ethical-ai-boston",
              "relation": "references"}], "source": "", "visibility": "internal",
              "audience": ["concept", "narrative", "liveops"], "risk_level": "critical"},
              "review": {"chain": [{"role": "content_director", "reviewer": "", "reviewed_at":
              "", "status": "pending"}], "next_actions": []}, "summary": {"problem":
              "В 2030-2039-х Гарвард запускает проект \"Этическая основа ИИ\" — попытку
              создать\nфилософскую и правовую рамку для развития искусственного интеллекта.
              По мере\nтого как ИИ становится все более мощным и автономным, возникает
              вопрос:\nможем ли мы контролировать создания, которые превосходят нас
              по интеллекту,\nили человечество обречено стать второстепенным видом
              в собственной цивилизации?\n", "goal": "Участвовать в разработке этической
              основы ИИ Гарварда, создать рамки\nдля безопасного и морального развития
              ИИ, обеспечить гармоничное сосуществование\nчеловека и машины.\n", "essence":
              "Квест исследует саму суть сознания и морали: может ли машина обладать\nистинной
              этикой, или все моральные суждения ИИ будут лишь отражением\nпрограммирования,
              заложенного людьми, чьи собственные моральные принципы\nчасто оказываются
              противоречивыми?\n", "key_points": ["Разработка философской основы ИИ",
              "Создание этических рамок для автономных систем", "Решение вопросов
              ответственности ИИ", "Определение границ человеческого контроля"]},
              "quest_definition": {"quest_type": "main", "level_min": 50, "level_max":
              60, "requirements": {"required_quests": [], "required_flags": ["boston_access",
              "philosophical_training"], "required_reputation": {}, "required_items":
              []}, "objectives": [{"id": "analyze_ai_moral_reasoning", "text": "Проанализировать
              моральное мышление ИИ", "type": "analyze", "target": "artificial_moral_frameworks",
              "count": 2, "optional": false}, {"id": "develop_ethical_guidelines",
              "text": "Разработать этические рамки", "type": "develop", "target":
              "ai_behavioral_standards", "count": 3, "optional": false}, {"id": "establish_accountability_mechanisms",
              "text": "Установить механизмы ответственности", "type": "establish",
              "target": "ai_liability_protocols", "count": 1, "optional": false},
              {"id": "define_human_ai_boundaries", "text": "Определить границы человек-ИИ",
              "type": "define", "target": "human_machine_relationship_framework",
              "count": 1, "optional": false}, {"id": "create_regulatory_framework",
              "text": "Создать регуляторную основу", "type": "create", "target": "ai_governance_legislation",
              "count": 1, "optional": false}], "rewards": [{"experience": 11600},
              {"credits": 27500}, {"reputation": {"factions": {"harvard_ethics_researchers":
              70, "ai_safety_advocates": 65, "philosophical_society_members": 60}}},
              {"items": ["ethical_ai_framework_manual", "ai_governance_credentials",
              "philosophical_reasoning_device"]}, {"unlocks": ["quest_ethical_ai_chain",
              "ai_ethics_research_skills", "philosophical_analysis_training"]}], "branches":
              [{"condition": "embrace_ai_superiority", "next_quest": "post_human_civilization"},
              {"condition": "establish_human_oversight", "next_quest": "controlled_ai_development"},
              {"condition": "restrict_ai_development", "next_quest": "technological_stagnation"}]},
              "content": {"sections": [{"id": "introduction", "title": "Введение",
              "body": "Бостон 2030-2039-х. Гарвард собирает конференцию \"Этическая
              основа ИИ\" —\nсобытие, которое должно определить будущее отношений
              человека и машины.\n\"ИИ может думать, чувствовать, решать,\" — говорят
              разработчики. \"Но может ли\nИИ быть моральным?\" — спрашивают философы.
              Кампусы Гарварда становятся\nареной дебатов о природе сознания и ответственности.\n"},
              {"id": "ai_consciousness_debate", "title": "Дебаты о сознании ИИ", "body":
              "Обладает ли ИИ истинным сознанием или лишь симулирует его? Может ли
              машина\nиспытывать эмоции, страдать, принимать моральные решения? \"Это
              просто\nалгоритмы,\" — говорят скептики. \"Это новая форма жизни,\"
              — настаивают\nэнтузиасты.\n"}, {"id": "moral_reasoning_in_machines",
              "title": "Моральное мышление в машинах", "body": "Как научить ИИ различать
              добро и зло? Стоит ли программировать мораль\nили позволить ИИ развивать
              свою этику? \"Мы должны запрограммировать\nбезопасность,\" — говорят
              инженеры. \"Мораль нельзя запрограммировать,\"\n— отвечают философы.\n"},
              {"id": "human_ai_relationships", "title": "Отношения человек-ИИ", "body":
              "Как изменятся человеческие отношения в мире, где ИИ может быть другом,\nлюбовником,
              наставником? \"Это расширение возможностей,\" — говорят оптимисты.\n\"Это
              конец аутентичных отношений,\" — отвечают традиционалисты.\n"}, {"id":
              "governance_and_control", "title": "Управление и контроль", "body":
              "Кто контролирует ИИ: правительства, корпорации, сами ИИ? Как предотвратить\nзлоупотребления?
              \"Нужен глобальный контроль,\" — говорят политики.\n\"Свобода инноваций
              превыше всего,\" — отвечают предприниматели.\n"}, {"id": "ethical_ai_decision",
              "title": "Решение по этическому ИИ", "body": "В зале заседаний факультета
              философии Гарварда, окруженный голографическими\nпроекциями этических
              дилемм ИИ, вы должны выбрать путь развития\nискусственного интеллекта:
              позволить ИИ развиваться свободно для достижения\nистинного сознания;
              ввести строгий человеческий контроль над развитием ИИ;\nили ограничить
              развитие ИИ для сохранения человеческого доминирования.\n"}]}, "appendix":
              {"glossary": [{"ethical_ai_framework": "этическая основа ИИ"}, {"ai_governance":
              "управление ИИ"}, {"moral_reasoning": "моральное мышление"}], "references":
              [], "decisions": ["ai_autonomy_vs_human_control", "machine_consciousness_vs_programmed_behavior",
              "technological_progress_vs_moral_responsibility"]}, "implementation":
              {"github_issue": "#140890848", "needs_task": false, "queue_reference":
              [], "blockers": []}, "history": [{"version": "1.0.0", "date": "2025-12-23",
              "author": "content_writer", "changes": "Создан квест об Этической основе
              ИИ Гарварда в Бостоне 2030-2039-х."}], "validation": {"checksum": "",
              "schema_version": "1.0"}}'
        - column:
            name: metadata
            value: '{"id": "quest-boston-2030-2039-harvard-ethical-ai-framework",
              "title": "Boston 2030-2039 — Этическая основа ИИ Гарварда", "document_type":
              "canon", "category": "quests", "status": "draft", "version": "1.0.0",
              "last_updated": "2025-12-23T18:11:00+00:00", "concept_approved": false,
              "concept_reviewed_at": "", "owners": [{"role": "content_writer", "contact":
              "content@necp.game"}], "tags": ["boston", "harvard-ethical-ai-framework",
              "ai-ethics", "philosophical-framework", "ai-governance"], "topics":
              ["artificial_intelligence_ethics", "philosophical_framework_development",
              "ai_governance_and_regulation", "human_ai_relationships", "moral_reasoning_in_machines"],
              "related_systems": ["narrative-service", "quest-service", "character-service",
              "ai-service"], "related_documents": [{"id": "lore-harvard-ethical-ai-boston",
              "relation": "references"}], "source": "", "visibility": "internal",
              "audience": ["concept", "narrative", "liveops"], "risk_level": "critical"}'
        - column:
            name: source_file
            value: knowledge\canon\lore\timeline-author\quests\america\boston\2030-2039\quest-002-harvard-ethical-ai-framework.yaml
