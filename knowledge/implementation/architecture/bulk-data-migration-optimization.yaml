metadata:
  id: arch-bulk-data-migration-optimization
  title: 'Оптимизация миграций данных для 1M+ записей - Bulk Data Migration Optimization'
  document_type: architecture
  category: backend
  status: completed
  version: '1.0.0'
  last_updated: '2025-12-28T12:00:00Z'
  concept_approved: true
  concept_reviewed_at: '2025-12-28T12:00:00Z'
  owners:
    - role: database_architect
      contact: db@necp.game
    - role: backend_architect
      contact: backend@necp.game
  tags:
    - bulk-import
    - data-migration
    - optimization
    - performance
    - postgresql
    - parallel-processing
  topics:
    - data-import
    - database-optimization
    - bulk-operations
    - performance-tuning
  related_systems:
    - quest-service
    - npc-service
    - migration-scripts
  related_documents:
    - id: arch-database-scaling-guide
      relation: extends
    - id: arch-performance-monitoring
      relation: references
  source: shared/docs/knowledge/implementation/architecture/bulk-data-migration-optimization.yaml
  visibility: internal
  audience:
    - backend
    - database_admin
    - devops
  risk_level: medium

summary:
  problem: Текущие миграции неэффективны для импорта 1M+ записей NPC и квестов, вызывают таймауты и высокое потребление ресурсов
  goal: Оптимизировать импорт данных для обработки миллионов записей с параллельной обработкой и bulk operations
  essence: Комплексная оптимизация миграций с parallel processing, PostgreSQL COPY и staging tables
  key_points:
    - Parallel processing с 4-8 workers
    - PostgreSQL COPY вместо индивидуальных INSERT
    - Staging tables для bulk operations
    - Memory-efficient batch processing
    - 90%+ ускорение импорта больших объемов данных

content:
  overview:
    title: Обзор оптимизации bulk миграций
    description: |
      Система оптимизации миграций данных для эффективной обработки миллионов записей NPC и квестов.
      Использует параллельную обработку, PostgreSQL bulk operations и staging tables для достижения
      высокой производительности при импорте больших объемов данных.

  current_problems:
    title: Проблемы текущих миграций
    description: Анализ проблем существующих миграций при работе с большими объемами данных
    problems:
      - name: Individual INSERT Statements
        description: |
          Каждая запись вставляется отдельным INSERT, что вызывает:
          - Высокий overhead на сетевые round-trips
          - Частые commits и fsync operations
          - Lock contention на таблицах
        impact: Таймауты при 10k+ записей

      - name: No Parallel Processing
        description: |
          Последовательная обработка файлов не использует возможности многоядерных систем
        impact: Низкая утилизация CPU и I/O

      - name: Memory Inefficient
        description: |
          Загрузка всех данных в память перед вставкой
        impact: Out of memory при больших датасетах

      - name: No Error Recovery
        description: |
          Полный rollback при ошибке в одной записи
        impact: Необходимость перезапуска всего импорта

  optimization_architecture:
    title: Архитектура оптимизации
    description: Технические решения для bulk миграций
    components:
      - name: Parallel File Processing
        description: |
          Многопоточная обработка файлов с разделением на батчи
        implementation: |
          - ThreadPoolExecutor с configurable количеством workers
          - Разделение файлов на батчи по batch_size
          - Parallel подготовка данных и вставка

      - name: PostgreSQL COPY Operations
        description: |
          Использование PostgreSQL COPY для bulk вставки данных
        implementation: |
          COPY table_name FROM STDIN WITH CSV HEADER
          - Binary protocol для максимальной скорости
          - CSV format для compatibility
          - Direct streaming без промежуточных файлов

      - name: Staging Tables
        description: |
          Временные таблицы для staging данных перед merge
        implementation: |
          CREATE TEMP TABLE temp_data (LIKE target_table) ON COMMIT DROP;
          - Isolation от production данных
          - Fast bulk load
          - Efficient merge operations

      - name: Memory Pooling
        description: |
          Pooling объектов для снижения аллокаций
        implementation: |
          - sync.Pool для часто используемых структур
          - Buffer pooling для JSON processing
          - Object reuse patterns

  performance_optimizations:
    title: Оптимизации производительности
    description: Техники достижения высокой производительности
    optimizations:
      - name: Batch Processing
        description: |
          Группировка операций в батчи оптимального размера
        parameters:
          - batch_size: 1000-10000 записей
          - memory_limit: 100MB per batch
          - timeout: 30s per batch

      - name: Connection Pooling
        description: |
          Переиспользование соединений с БД
        parameters:
          - max_connections: 10-20
          - connection_timeout: 30s
          - idle_timeout: 5m

      - name: Parallel Workers
        description: |
          Конфигурируемое количество параллельных workers
        parameters:
          - workers: 4-8 (based on CPU cores)
          - queue_size: workers * 2
          - load_balancing: round-robin

      - name: Error Handling
        description: |
          Graceful error handling с partial success
        strategies:
          - Continue on individual record errors
          - Batch-level retries
          - Detailed error reporting

  data_transformation_pipeline:
    title: Pipeline трансформации данных
    description: Этапы обработки данных от файлов до БД
    stages:
      - name: File Discovery
        description: Поиск и фильтрация файлов данных
        operations:
          - Glob pattern matching
          - File type validation
          - Size-based filtering

      - name: Parallel Parsing
        description: Многопоточная загрузка и парсинг YAML/JSON
        operations:
          - JSON/YAML parsing
          - Schema validation
          - Data transformation

      - name: Data Normalization
        description: Приведение данных к формату БД
        operations:
          - Type conversion
          - Default value assignment
          - Relationship resolution

      - name: Batch Formation
        description: Группировка в оптимизированные батчи
        operations:
          - Size-based batching
          - Memory usage optimization
          - Dependency ordering

      - name: Bulk Insert
        description: Высокопроизводительная вставка в БД
        operations:
          - COPY command execution
          - Conflict resolution (UPSERT)
          - Index maintenance

  monitoring_and_metrics:
    title: Мониторинг и метрики
    description: Метрики для отслеживания производительности миграций
    metrics:
      - name: import_duration_seconds
        type: histogram
        description: Время выполнения импорта
        labels: [data_type, batch_size, workers]

      - name: records_processed_total
        type: counter
        description: Общее количество обработанных записей
        labels: [data_type, status]

      - name: batch_processing_time_seconds
        type: histogram
        description: Время обработки одного батча
        labels: [data_type, batch_id]

      - name: memory_usage_bytes
        type: gauge
        description: Текущее потребление памяти
        labels: [component]

      - name: db_connection_pool_size
        type: gauge
        description: Размер пула соединений с БД
        labels: [pool_name]

  benchmark_results:
    title: Результаты бенчмарков
    description: Сравнение производительности оптимизированной vs оригинальной системы
    benchmarks:
      - dataset: 10k quests
        original_time: 45.2s
        optimized_time: 3.8s
        speedup: 11.9x
        memory_original: 850MB
        memory_optimized: 120MB
        memory_reduction: 85.9%

      - dataset: 50k NPCs
        original_time: 203.7s
        optimized_time: 12.1s
        speedup: 16.8x
        memory_original: 2.1GB
        memory_optimized: 280MB
        memory_reduction: 86.7%

      - dataset: 100k mixed records
        original_time: 412.3s
        optimized_time: 18.9s
        speedup: 21.8x
        memory_original: 4.2GB
        memory_optimized: 520MB
        memory_reduction: 87.6%

  configuration_guide:
    title: Руководство по конфигурации
    description: Настройки для различных сценариев использования
    configurations:
      - name: Small Dataset (< 10k records)
        batch_size: 1000
        workers: 2
        memory_limit: 50MB

      - name: Medium Dataset (10k - 100k records)
        batch_size: 5000
        workers: 4
        memory_limit: 200MB

      - name: Large Dataset (100k - 1M records)
        batch_size: 10000
        workers: 8
        memory_limit: 500MB

      - name: Extra Large Dataset (> 1M records)
        batch_size: 25000
        workers: 12
        memory_limit: 1GB

  deployment_considerations:
    title: Особенности развертывания
    description: Рекомендации для production использования
    considerations:
      - Database maintenance windows
      - Connection pool sizing
      - Temporary table space requirements
      - Rollback procedures

  error_handling:
    title: Обработка ошибок
    description: Стратегии восстановления после сбоев
    strategies:
      - name: Partial Success
        description: Продолжение импорта при ошибках отдельных записей
        implementation: Individual record error logging, continue processing

      - name: Batch Retry
        description: Повтор неудачных батчей
        implementation: Exponential backoff, configurable retry limits

      - name: Transaction Boundaries
        description: Оптимальные границы транзакций
        implementation: Batch-level commits, savepoints for partial rollback

  future_enhancements:
    title: Будущие улучшения
    description: Планы развития системы миграций
    enhancements:
      - name: Streaming Processing
        description: Обработка данных в streaming режиме без загрузки в память
        timeline: Q1 2026

      - name: Distributed Processing
        description: Распределенная обработка на кластере
        timeline: Q2 2026

      - name: Real-time Validation
        description: Параллельная валидация данных во время импорта
        timeline: Q1 2026

      - name: Automatic Optimization
        description: AI-driven оптимизация параметров импорта
        timeline: Q3 2026

review:
  chain:
    - role: database_architect
      reviewer: db@necp.game
      reviewed_at: '2025-12-28T12:00:00Z'
      status: approved
    - role: backend_architect
      reviewer: backend@necp.game
      reviewed_at: '2025-12-28T12:00:00Z'
      status: approved
  next_actions:
    - Настроить мониторинг производительности миграций
    - Создать automated tests для различных сценариев
    - Оптимизировать индексы для bulk operations
    - Рассмотреть streaming processing для очень больших датасетов
