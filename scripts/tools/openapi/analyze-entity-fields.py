#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ø–æ–ª–µ–π —Å—É—â–Ω–æ—Å—Ç–µ–π –≤ OpenAPI —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è—Ö.

–¶–µ–ª—å: –ù–∞–π—Ç–∏ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ–ª—è –¥–ª—è –º–∏–≥—Ä–∞—Ü–∏–∏ –Ω–∞ BASE-ENTITY —Å–∏—Å—Ç–µ–º—É.

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python scripts/openapi/analyze-entity-fields.py proto/openapi/social-domain/
    python scripts/openapi/analyze-entity-fields.py proto/openapi/ --all-domains
"""

import os
import yaml
import json
import argparse
from collections import defaultdict, Counter
from pathlib import Path
from typing import Dict, List, Set, Tuple

class EntityFieldAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ø–æ–ª–µ–π —Å—É—â–Ω–æ—Å—Ç–µ–π –¥–ª—è DRY compliance."""

    def __init__(self):
        self.field_usage = defaultdict(lambda: defaultdict(int))  # field -> entity -> count
        self.entity_fields = defaultdict(set)  # entity -> set of fields
        self.common_fields = set()
        self.duplicate_patterns = defaultdict(list)

    def analyze_file(self, file_path: str) -> None:
        """–ê–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–≥–æ YAML —Ñ–∞–π–ª–∞."""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = yaml.safe_load(f)

            self._extract_schemas(content, file_path)

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {file_path}: {e}")

    def _extract_schemas(self, content: dict, file_path: str) -> None:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ö–µ–º –∏–∑ OpenAPI —Ñ–∞–π–ª–∞."""
        if not isinstance(content, dict):
            return

        # –ò—â–µ–º components.schemas
        components = content.get('components', {})
        schemas = components.get('schemas', {})

        for schema_name, schema_def in schemas.items():
            if isinstance(schema_def, dict) and 'properties' in schema_def:
                self._analyze_entity_properties(schema_name, schema_def, file_path)

    def _analyze_entity_properties(self, entity_name: str, schema: dict, file_path: str) -> None:
        """–ê–Ω–∞–ª–∏–∑ —Å–≤–æ–π—Å—Ç–≤ —Å—É—â–Ω–æ—Å—Ç–∏."""
        properties = schema.get('properties', {})

        for field_name, field_def in properties.items():
            self.field_usage[field_name][entity_name] += 1
            self.entity_fields[entity_name].add(field_name)

    def find_common_fields(self, min_occurrences: int = 3) -> Set[str]:
        """–ù–∞–π—Ç–∏ —á–∞—Å—Ç–æ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –ø–æ–ª—è."""
        common_fields = set()

        for field, entities in self.field_usage.items():
            if len(entities) >= min_occurrences:
                common_fields.add(field)

        self.common_fields = common_fields
        return common_fields

    def detect_duplicate_patterns(self) -> Dict[str, List[str]]:
        """–ù–∞–π—Ç–∏ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è."""
        patterns = defaultdict(list)

        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –Ω–∞–±–æ—Ä—É –ø–æ–ª–µ–π
        field_sets = defaultdict(list)
        for entity, fields in self.entity_fields.items():
            field_key = frozenset(fields)
            field_sets[field_key].append(entity)

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã —Å –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–º–∏—Å—è –Ω–∞–±–æ—Ä–∞–º–∏ –ø–æ–ª–µ–π
        for field_set, entities in field_sets.items():
            if len(entities) > 1 and len(field_set) > 3:  # –ú–∏–Ω–∏–º—É–º 3 –ø–æ–ª—è –∏ 2 —Å—É—â–Ω–æ—Å—Ç–∏
                pattern_key = f"pattern_{len(field_set)}_fields"
                patterns[pattern_key] = entities

        self.duplicate_patterns = patterns
        return patterns

    def calculate_dry_metrics(self) -> Dict[str, float]:
        """–†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ DRY compliance."""
        total_fields = sum(len(fields) for fields in self.entity_fields.values())
        unique_fields = len(set().union(*self.entity_fields.values()))
        duplicate_fields = total_fields - unique_fields

        # –ü—Ä–æ—Ü–µ–Ω—Ç –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è
        duplication_rate = (duplicate_fields / total_fields * 100) if total_fields > 0 else 0

        # –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ª–µ–π –Ω–∞ —Å—É—â–Ω–æ—Å—Ç—å
        avg_fields_per_entity = total_fields / len(self.entity_fields) if self.entity_fields else 0

        return {
            'total_fields': total_fields,
            'unique_fields': unique_fields,
            'duplicate_fields': duplicate_fields,
            'duplication_rate_percent': duplication_rate,
            'avg_fields_per_entity': avg_fields_per_entity,
            'entities_count': len(self.entity_fields),
            'common_fields_count': len(self.common_fields)
        }

    def generate_report(self) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞."""
        metrics = self.calculate_dry_metrics()
        self.find_common_fields()
        self.detect_duplicate_patterns()

        report = []
        report.append("# üìä –ê–ù–ê–õ–ò–ó –ü–û–õ–ï–ô –°–£–©–ù–û–°–¢–ï–ô - DRY COMPLIANCE REPORT")
        report.append("")
        report.append("## üìà –ú–ï–¢–†–ò–ö–ò –î–£–ë–õ–ò–†–û–í–ê–ù–ò–Ø")
        report.append("")
        report.append(f"- **–í—Å–µ–≥–æ –ø–æ–ª–µ–π:** {metrics['total_fields']}")
        report.append(f"- **–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π:** {metrics['unique_fields']}")
        report.append(f"- **–î—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ–ª–µ–π:** {metrics['duplicate_fields']}")
        report.append(".1f")
        report.append(".1f")
        report.append(f"- **–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—É—â–Ω–æ—Å—Ç–µ–π:** {metrics['entities_count']}")
        report.append(f"- **–û–±—â–∏—Ö –ø–æ–ª–µ–π (3+ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è):** {metrics['common_fields_count']}")
        report.append("")

        if self.common_fields:
            report.append("## üîç –ù–ê–ò–ë–û–õ–ï–ï –ß–ê–°–¢–û –ò–°–ü–û–õ–¨–ó–£–ï–ú–´–ï –ü–û–õ–Ø")
            report.append("")
            field_counts = [(field, len(entities)) for field, entities in self.field_usage.items()]
            field_counts.sort(key=lambda x: x[1], reverse=True)

            for field, count in field_counts[:20]:  # –¢–æ–ø 20
                entities = list(self.field_usage[field].keys())
                report.append(f"- `{field}`: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ {count} —Å—É—â–Ω–æ—Å—Ç—è—Ö")
                if len(entities) <= 5:
                    report.append(f"  - –°—É—â–Ω–æ—Å—Ç–∏: {', '.join(entities)}")
                else:
                    report.append(f"  - –°—É—â–Ω–æ—Å—Ç–∏: {', '.join(entities[:3])}, ... (+{len(entities)-3})")
            report.append("")

        if self.duplicate_patterns:
            report.append("## üéØ –ü–ê–¢–¢–ï–†–ù–´ –î–£–ë–õ–ò–†–û–í–ê–ù–ò–Ø")
            report.append("")
            for pattern, entities in list(self.duplicate_patterns.items())[:10]:  # –¢–æ–ø 10 –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
                report.append(f"- **{pattern}**: {len(entities)} —Å—É—â–Ω–æ—Å—Ç–µ–π")
                report.append(f"  - –°—É—â–Ω–æ—Å—Ç–∏: {', '.join(entities)}")
            report.append("")

        report.append("## üí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –î–õ–Ø BASE-ENTITY")
        report.append("")
        report.append("### –ü—Ä–µ–¥–ª–∞–≥–∞–µ–º—ã–µ BASE-ENTITY –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:")
        report.append("")
        base_entity_candidates = self._analyze_base_entity_candidates()
        if base_entity_candidates:
            for candidate in base_entity_candidates[:5]:
                report.append(f"- **{candidate['name']}**: {candidate['entities']} —Å—É—â–Ω–æ—Å—Ç–µ–π")
                report.append(f"  - –ü–æ–ª—è: {', '.join(list(candidate['fields'])[:10])}")
                if len(candidate['fields']) > 10:
                    report.append(f"    ... (+{len(candidate['fields'])-10} –ø–æ–ª–µ–π)")
        report.append("")
        report.append("### –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:")
        report.append("1. –°–æ–∑–¥–∞—Ç—å BASE-ENTITY –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞")
        report.append("2. –ú–∏–≥—Ä–∞—Ü–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π –Ω–∞ allOf –∫–æ–º–ø–æ–∑–∏—Ü–∏—é")
        report.append("3. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ $ref —Å—Å—ã–ª–æ–∫")
        report.append("4. –í–∞–ª–∏–¥–∞—Ü–∏—è —á–µ—Ä–µ–∑ ogen")

        return "\n".join(report)

    def _analyze_base_entity_candidates(self) -> List[Dict]:
        """–ê–Ω–∞–ª–∏–∑ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è BASE-ENTITY."""
        candidates = []

        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –æ–±—â–∏–º –ø–æ–ª—è–º
        field_combinations = defaultdict(set)

        for entity, fields in self.entity_fields.items():
            # –ù–∞—Ö–æ–¥–∏–º –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –ø–æ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è —á–∞—Å—Ç–æ
            common_entity_fields = fields.intersection(self.common_fields)
            if len(common_entity_fields) >= 3:  # –ú–∏–Ω–∏–º—É–º 3 –æ–±—â–∏—Ö –ø–æ–ª—è
                key = frozenset(common_entity_fields)
                field_combinations[key].add(entity)

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
        for field_set, entities in field_combinations.items():
            if len(entities) >= 2:  # –ú–∏–Ω–∏–º—É–º 2 —Å—É—â–Ω–æ—Å—Ç–∏
                candidates.append({
                    'name': f"BaseEntity_{len(field_set)}_fields",
                    'fields': field_set,
                    'entities': list(entities),
                    'entity_count': len(entities),
                    'field_count': len(field_set)
                })

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å—É—â–Ω–æ—Å—Ç–µ–π –∏ –ø–æ–ª–µ–π
        candidates.sort(key=lambda x: (x['entity_count'], x['field_count']), reverse=True)

        return candidates


def main():
    parser = argparse.ArgumentParser(description='–ê–Ω–∞–ª–∏–∑ –ø–æ–ª–µ–π —Å—É—â–Ω–æ—Å—Ç–µ–π OpenAPI –¥–ª—è DRY compliance')
    parser.add_argument('path', help='–ü—É—Ç—å –∫ –¥–æ–º–µ–Ω—É –∏–ª–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å YAML —Ñ–∞–π–ª–∞–º–∏')
    parser.add_argument('--all-domains', action='store_true', help='–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ –¥–æ–º–µ–Ω—ã')
    parser.add_argument('--output', '-o', default='scripts/reports/entity-analysis-report.md', help='–§–∞–π–ª –¥–ª—è –æ—Ç—á–µ—Ç–∞')
    parser.add_argument('--min-occurrences', type=int, default=3, help='–ú–∏–Ω–∏–º—É–º –≤—Ö–æ–∂–¥–µ–Ω–∏–π –¥–ª—è common fields')

    args = parser.parse_args()

    analyzer = EntityFieldAnalyzer()

    if args.all_domains:
        # –ê–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö –¥–æ–º–µ–Ω–æ–≤
        domains_path = Path(args.path)
        if domains_path.exists():
            for domain_dir in domains_path.iterdir():
                if domain_dir.is_dir() and not domain_dir.name.startswith('.'):
                    print(f"–ê–Ω–∞–ª–∏–∑ –¥–æ–º–µ–Ω–∞: {domain_dir.name}")
                    for yaml_file in domain_dir.rglob('*.yaml'):
                        analyzer.analyze_file(str(yaml_file))
    else:
        # –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø—É—Ç–∏
        path = Path(args.path)
        if path.is_dir():
            for yaml_file in path.rglob('*.yaml'):
                analyzer.analyze_file(str(yaml_file))
        elif path.is_file():
            analyzer.analyze_file(str(path))

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
    report = analyzer.generate_report()

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
    with open(args.output, 'w', encoding='utf-8') as f:
        f.write(report)

    print(f"[OK] Analysis completed. Report saved to: {args.output}")

    # –í—ã–≤–æ–¥ –æ—Å–Ω–æ–≤–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫ –≤ –∫–æ–Ω—Å–æ–ª—å
    metrics = analyzer.calculate_dry_metrics()
    print("\n[INFO] MAIN METRICS:")
    print(f"   Total fields: {metrics['total_fields']}")
    print(".1f")
    print(f"   Unique fields: {metrics['unique_fields']}")
    print(f"   Duplicate fields: {metrics['duplicate_fields']}")
    print(f"   Entities: {metrics['entities_count']}")


if __name__ == '__main__':
    main()
