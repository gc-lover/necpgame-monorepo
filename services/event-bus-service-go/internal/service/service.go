// Code generated by NECPGAME backend agent. Enterprise-grade Event Bus service.
// PERFORMANCE: Optimized for real-time event processing and Kafka message handling
// Issue: #2237 - Kafka Event-Driven Architecture

package service

import (
	"context"
	"encoding/json"
	"fmt"
	"strings"
	"sync"
	"time"

	"github.com/Shopify/sarama"
	"github.com/go-faster/errors"
	"github.com/google/uuid"
	"github.com/jackc/pgx/v5/pgxpool"
	"github.com/prometheus/client_golang/prometheus"
	"go.opentelemetry.io/otel/metric"
	"go.opentelemetry.io/otel/trace"
	"go.uber.org/zap"

	"necpgame/services/event-bus-service-go/internal/models"
)

// PERFORMANCE: Memory pooling for hot path objects (Level 2 optimization)
// Reduces GC pressure in high-throughput event processing
var (
	eventPool = sync.Pool{
		New: func() interface{} {
			return &models.Event{}
		},
	}

	gameEventPool = sync.Pool{
		New: func() interface{} {
			return &models.GameEvent{}
		},
	}

	systemEventPool = sync.Pool{
		New: func() interface{} {
			return &models.SystemEvent{}
		},
	}
)

// Config holds service configuration
// PERFORMANCE: Struct field alignment optimized for memory efficiency
type Config struct {
	Logger       *zap.Logger
	Tracer       trace.Tracer
	Meter        metric.Meter
	DatabaseURL  string
	RedisURL     string
	KafkaBrokers string
}

// EventBusService implements enterprise-grade event-driven architecture
// PERFORMANCE: Struct field alignment optimized for memory efficiency
type EventBusService struct {
	// Core dependencies (pointers first)
	logger        *zap.Logger
	tracer        trace.Tracer
	meter         metric.Meter
	db            *pgxpool.Pool
	redis         *redis.Client

	// Kafka components
	kafkaProducer sarama.SyncProducer
	kafkaConsumer sarama.ConsumerGroup
	kafkaAdmin    sarama.ClusterAdmin

	// Event processing
	eventBuffer   chan *models.Event
	workerPool    chan func()
	workersWG     sync.WaitGroup

	// Topic management
	topics        map[string]*models.Topic
	topicsMutex   sync.RWMutex

	// Subscription management
	subscriptions map[string]*models.EventSubscription
	subsMutex     sync.RWMutex

	// Configuration
	config        *EventBusConfig

	// Prometheus metrics
	eventsPublished      *prometheus.CounterVec
	eventsConsumed       *prometheus.CounterVec
	eventProcessingTime  *prometheus.HistogramVec
	kafkaOperations      *prometheus.CounterVec
	eventBufferSize      prometheus.Gauge
	activeWorkers        prometheus.Gauge
	databaseQueryTime    *prometheus.HistogramVec
	redisOperationTime   *prometheus.HistogramVec
	goroutineCount       prometheus.Gauge
	gcPauseDuration      prometheus.Histogram
}

// EventBusConfig holds event bus configuration
type EventBusConfig struct {
	WorkerPoolSize    int
	EventBufferSize   int
	BatchSize         int
	MaxRetryAttempts  int
	RetryBackoff      time.Duration
	DefaultPartitions int
	DefaultReplicas   int
}

// NewEventBusService creates optimized event bus service instance
func NewEventBusService(cfg Config) (*EventBusService, error) {
	svc := &EventBusService{
		logger: cfg.Logger,
		tracer: cfg.Tracer,
		meter:  cfg.Meter,

		// Event processing configuration
		config: &EventBusConfig{
			WorkerPoolSize:    10,
			EventBufferSize:   1000,
			BatchSize:         100,
			MaxRetryAttempts:  3,
			RetryBackoff:      1 * time.Second,
			DefaultPartitions: 3,
			DefaultReplicas:   2,
		},

		// Initialize maps
		topics:        make(map[string]*models.Topic),
		subscriptions: make(map[string]*models.EventSubscription),

		// Initialize channels
		eventBuffer: make(chan *models.Event, 1000),
		workerPool:  make(chan func(), 10),
	}

	// Initialize Prometheus metrics
	svc.initMetrics()

	// Initialize database with performance optimizations
	if cfg.DatabaseURL != "" {
		if err := svc.initDatabase(cfg.DatabaseURL); err != nil {
			return nil, errors.Wrap(err, "failed to init database")
		}
	}

	// Initialize Redis with performance optimizations
	if cfg.RedisURL != "" {
		if err := svc.initRedis(cfg.RedisURL); err != nil {
			return nil, errors.Wrap(err, "failed to init redis")
		}
	}

	// Initialize Kafka with performance optimizations
	if cfg.KafkaBrokers != "" {
		if err := svc.initKafka(strings.Split(cfg.KafkaBrokers, ",")); err != nil {
			return nil, errors.Wrap(err, "failed to init kafka")
		}
	}

	// Start event processing workers
	svc.startWorkers()

	svc.logger.Info("Event bus service initialized successfully",
		zap.String("optimization_level", "Level 2 (Event-Driven)"),
		zap.Bool("struct_alignment", true),
		zap.Bool("memory_pooling", true),
		zap.Bool("kafka_integration", true))
	return svc, nil
}

// initMetrics initializes Prometheus metrics
func (s *EventBusService) initMetrics() {
	s.eventsPublished = prometheus.NewCounterVec(
		prometheus.CounterOpts{
			Name: "event_bus_events_published_total",
			Help: "Total number of events published",
		},
		[]string{"topic", "event_type", "status"},
	)

	s.eventsConsumed = prometheus.NewCounterVec(
		prometheus.CounterOpts{
			Name: "event_bus_events_consumed_total",
			Help: "Total number of events consumed",
		},
		[]string{"topic", "event_type", "status"},
	)

	s.eventProcessingTime = prometheus.NewHistogramVec(
		prometheus.HistogramOpts{
			Name:    "event_bus_event_processing_duration_seconds",
			Help:    "Event processing duration in seconds",
			Buckets: prometheus.DefBuckets,
		},
		[]string{"operation", "event_type"},
	)

	s.kafkaOperations = prometheus.NewCounterVec(
		prometheus.CounterOpts{
			Name: "event_bus_kafka_operations_total",
			Help: "Total number of Kafka operations",
		},
		[]string{"operation", "status"},
	)

	s.eventBufferSize = prometheus.NewGauge(
		prometheus.GaugeOpts{
			Name: "event_bus_event_buffer_size",
			Help: "Current size of event buffer",
		},
	)

	s.activeWorkers = prometheus.NewGauge(
		prometheus.GaugeOpts{
			Name: "event_bus_active_workers",
			Help: "Number of active worker goroutines",
		},
	)

	s.databaseQueryTime = prometheus.NewHistogramVec(
		prometheus.HistogramOpts{
			Name:    "event_bus_database_query_duration_seconds",
			Help:    "Database query duration in seconds",
			Buckets: prometheus.DefBuckets,
		},
		[]string{"operation"},
	)

	s.redisOperationTime = prometheus.NewHistogramVec(
		prometheus.HistogramOpts{
			Name:    "event_bus_redis_operation_duration_seconds",
			Help:    "Redis operation duration in seconds",
			Buckets: []float64{0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1},
		},
		[]string{"operation"},
	)

	s.goroutineCount = prometheus.NewGauge(
		prometheus.GaugeOpts{
			Name: "event_bus_goroutines",
			Help: "Number of active goroutines",
		},
	)

	s.gcPauseDuration = prometheus.NewHistogram(
		prometheus.HistogramOpts{
			Name:    "event_bus_gc_pause_duration_seconds",
			Help:    "GC pause duration in seconds",
			Buckets: []float64{0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1},
		},
	)

	// Register metrics
	prometheus.MustRegister(
		s.eventsPublished,
		s.eventsConsumed,
		s.eventProcessingTime,
		s.kafkaOperations,
		s.eventBufferSize,
		s.activeWorkers,
		s.databaseQueryTime,
		s.redisOperationTime,
		s.goroutineCount,
		s.gcPauseDuration,
	)
}

// initDatabase initializes PostgreSQL connection with performance optimizations
func (s *EventBusService) initDatabase(databaseURL string) error {
	config, err := pgxpool.ParseConfig(databaseURL)
	if err != nil {
		return errors.Wrap(err, "failed to parse database URL")
	}

	// PERFORMANCE: Optimized for event-driven operations
	config.MaxConns = 30                    // Higher for event processing
	config.MinConns = 10                    // Keep connections ready
	config.MaxConnLifetime = 30 * time.Minute // Longer for event streams
	config.MaxConnIdleTime = 5 * time.Minute  // Moderate cleanup

	pool, err := pgxpool.NewWithConfig(context.Background(), config)
	if err != nil {
		return errors.Wrap(err, "failed to create connection pool")
	}

	ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
	defer cancel()

	if err := pool.Ping(ctx); err != nil {
		return errors.Wrap(err, "failed to ping database")
	}

	s.db = pool
	s.logger.Info("Database connection established with event-driven optimizations",
		zap.Int("max_conns", 30),
		zap.Int("min_conns", 10))
	return nil
}

// initRedis initializes Redis connection for caching and session storage
func (s *EventBusService) initRedis(redisURL string) error {
	opt, err := redis.ParseURL(redisURL)
	if err != nil {
		return errors.Wrap(err, "failed to parse redis URL")
	}

	rdb := redis.NewClient(opt)
	rdb.Options().PoolSize = 20           // Higher for event caching
	rdb.Options().MinIdleConns = 5        // Keep connections ready
	rdb.Options().ConnMaxLifetime = 20 * time.Minute // Match event lifetime

	ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
	defer cancel()

	if err := rdb.Ping(ctx).Err(); err != nil {
		return errors.Wrap(err, "failed to ping redis")
	}

	s.redis = rdb
	s.logger.Info("Redis connection established with event-driven optimizations",
		zap.Int("pool_size", 20),
		zap.Int("min_idle", 5))
	return nil
}

// initKafka initializes Kafka producer, consumer, and admin client
func (s *EventBusService) initKafka(brokers []string) error {
	// Producer configuration
	producerConfig := sarama.NewConfig()
	producerConfig.Producer.RequiredAcks = sarama.WaitForAll
	producerConfig.Producer.Retry.Max = 3
	producerConfig.Producer.Return.Successes = true
	producerConfig.Producer.Timeout = 10 * time.Second
	producerConfig.Producer.Flush.Frequency = 500 * time.Millisecond
	producerConfig.Producer.Flush.Messages = 100

	producer, err := sarama.NewSyncProducer(brokers, producerConfig)
	if err != nil {
		return errors.Wrap(err, "failed to create kafka producer")
	}
	s.kafkaProducer = producer

	// Admin client for topic management
	admin, err := sarama.NewClusterAdmin(brokers, producerConfig)
	if err != nil {
		return errors.Wrap(err, "failed to create kafka admin")
	}
	s.kafkaAdmin = admin

	// Consumer group configuration
	consumerConfig := sarama.NewConfig()
	consumerConfig.Consumer.Group.Rebalance.Strategy = sarama.BalanceStrategyRoundRobin
	consumerConfig.Consumer.Offsets.AutoCommit.Enable = true
	consumerConfig.Consumer.Offsets.AutoCommit.Interval = 1 * time.Second
	consumerConfig.Consumer.Offsets.Initial = sarama.OffsetNewest

	consumer, err := sarama.NewConsumerGroup(brokers, "event-bus-service", consumerConfig)
	if err != nil {
		return errors.Wrap(err, "failed to create kafka consumer")
	}
	s.kafkaConsumer = consumer

	s.logger.Info("Kafka connections established",
		zap.Strings("brokers", brokers),
		zap.Bool("producer_ready", true),
		zap.Bool("consumer_ready", true),
		zap.Bool("admin_ready", true))
	return nil
}

// startWorkers starts the worker pool for event processing
func (s *EventBusService) startWorkers() {
	s.activeWorkers.Set(float64(s.config.WorkerPoolSize))

	for i := 0; i < s.config.WorkerPoolSize; i++ {
		s.workersWG.Add(1)
		go s.worker(i)
	}

	s.logger.Info("Event processing workers started",
		zap.Int("worker_count", s.config.WorkerPoolSize))
}

// worker processes events from the buffer
func (s *EventBusService) worker(id int) {
	defer s.workersWG.Done()

	for {
		select {
		case work := <-s.workerPool:
			s.activeWorkers.Inc()
			work()
			s.activeWorkers.Dec()
		}
	}
}

// PublishEvent publishes an event to Kafka
func (s *EventBusService) PublishEvent(ctx context.Context, event *models.Event) error {
	start := time.Now()
	defer func() {
		s.eventProcessingTime.WithLabelValues("publish", event.EventType).Observe(time.Since(start).Seconds())
	}()

	ctx, cancel := context.WithTimeout(ctx, 10*time.Second)
	defer cancel()

	// Validate event
	if err := s.validateEvent(event); err != nil {
		s.eventsPublished.WithLabelValues(event.Topic, event.EventType, "invalid").Inc()
		return errors.Wrap(err, "invalid event")
	}

	// Generate event ID if not provided
	if event.EventID == "" {
		event.EventID = uuid.New().String()
	}
	if event.ID == "" {
		event.ID = uuid.New().String()
	}

	// Set timestamps
	event.Timestamp = time.Now().UTC()
	event.CreatedAt = event.Timestamp

	// Convert event to JSON
	eventJSON, err := json.Marshal(event)
	if err != nil {
		s.eventsPublished.WithLabelValues(event.Topic, event.EventType, "marshal_error").Inc()
		return errors.Wrap(err, "failed to marshal event")
	}

	// Create Kafka message
	message := &sarama.ProducerMessage{
		Topic: event.Topic,
		Key:   sarama.StringEncoder(event.EventID),
		Value: sarama.StringEncoder(eventJSON),
		Headers: []sarama.RecordHeader{
			{Key: []byte("event_type"), Value: []byte(event.EventType)},
			{Key: []byte("source"), Value: []byte(event.Source)},
			{Key: []byte("timestamp"), Value: []byte(event.Timestamp.Format(time.RFC3339))},
		},
	}

	// Send to Kafka
	partition, offset, err := s.kafkaProducer.SendMessage(message)
	if err != nil {
		s.kafkaOperations.WithLabelValues("publish", "error").Inc()
		s.eventsPublished.WithLabelValues(event.Topic, event.EventType, "publish_error").Inc()
		return errors.Wrap(err, "failed to publish event to kafka")
	}

	// Update event metadata
	event.Partition = int(partition)
	event.Offset = offset
	event.Status = "published"

	// Store event in database for audit and replay
	if err := s.storeEvent(ctx, event); err != nil {
		s.logger.Warn("Failed to store event in database", zap.Error(err))
	}

	s.kafkaOperations.WithLabelValues("publish", "success").Inc()
	s.eventsPublished.WithLabelValues(event.Topic, event.EventType, "success").Inc()

	s.logger.Info("Event published successfully",
		zap.String("event_id", event.EventID),
		zap.String("topic", event.Topic),
		zap.String("event_type", event.EventType),
		zap.Int32("partition", partition),
		zap.Int64("offset", offset))

	return nil
}

// PublishGameEvent publishes a game-specific event
func (s *EventBusService) PublishGameEvent(ctx context.Context, gameEvent *models.GameEvent) error {
	// Convert to generic event
	event := &models.Event{
		EventID:     gameEvent.EventID,
		EventType:   gameEvent.EventType,
		Topic:       gameEvent.Topic,
		Source:      gameEvent.Source,
		Payload:     "", // Will be set by JSON marshaling
		Timestamp:   gameEvent.Timestamp,
		Priority:    "normal",
		IsReplayable: true,
		IsEncrypted:  false,
	}

	// Set game-specific payload
	payload := map[string]interface{}{
		"player_id":   gameEvent.PlayerID,
		"session_id":  gameEvent.SessionID,
		"game_mode":   gameEvent.GameMode,
		"map_name":    gameEvent.MapName,
		"action_type": gameEvent.ActionType,
		"action_data": gameEvent.ActionData,
		"x":           gameEvent.X,
		"y":           gameEvent.Y,
		"z":           gameEvent.Z,
		"timestamp":   gameEvent.Timestamp,
		"server_id":   gameEvent.ServerID,
		"region":      gameEvent.Region,
		"version":     gameEvent.Version,
	}

	payloadJSON, err := json.Marshal(payload)
	if err != nil {
		return errors.Wrap(err, "failed to marshal game event payload")
	}
	event.Payload = string(payloadJSON)

	return s.PublishEvent(ctx, event)
}

// CreateTopic creates a new Kafka topic
func (s *EventBusService) CreateTopic(ctx context.Context, topic *models.Topic) error {
	ctx, cancel := context.WithTimeout(ctx, 15*time.Second)
	defer cancel()

	// Validate topic configuration
	if err := s.validateTopic(topic); err != nil {
		return errors.Wrap(err, "invalid topic configuration")
	}

	// Set defaults
	if topic.Partitions == 0 {
		topic.Partitions = s.config.DefaultPartitions
	}
	if topic.ReplicationFactor == 0 {
		topic.ReplicationFactor = s.config.DefaultReplicas
	}

	// Create Kafka topic
	topicDetail := &sarama.TopicDetail{
		NumPartitions:     int32(topic.Partitions),
		ReplicationFactor: int16(topic.ReplicationFactor),
		ConfigEntries: map[string]*string{
			"retention.ms": &[]string{fmt.Sprintf("%d", topic.RetentionHours*3600000)}[0],
		},
	}

	topicDetails := map[string]*sarama.TopicDetail{
		topic.Name: topicDetail,
	}

	err := s.kafkaAdmin.CreateTopics(topicDetails, false)
	if err != nil {
		s.kafkaOperations.WithLabelValues("create_topic", "error").Inc()
		return errors.Wrap(err, "failed to create kafka topic")
	}

	// Store topic metadata
	topic.ID = uuid.New().String()
	topic.Status = "active"
	topic.CreatedAt = time.Now().UTC()
	topic.UpdatedAt = topic.CreatedAt

	if err := s.storeTopic(ctx, topic); err != nil {
		s.logger.Warn("Failed to store topic metadata", zap.Error(err))
	}

	// Cache topic
	s.topicsMutex.Lock()
	s.topics[topic.Name] = topic
	s.topicsMutex.Unlock()

	s.kafkaOperations.WithLabelValues("create_topic", "success").Inc()
	s.logger.Info("Topic created successfully",
		zap.String("topic", topic.Name),
		zap.Int("partitions", topic.Partitions),
		zap.Int("replicas", topic.ReplicationFactor))

	return nil
}

// SubscribeToEvents creates an event subscription
func (s *EventBusService) SubscribeToEvents(ctx context.Context, subscription *models.EventSubscription) error {
	ctx, cancel := context.WithTimeout(ctx, 10*time.Second)
	defer cancel()

	// Validate subscription
	if err := s.validateSubscription(subscription); err != nil {
		return errors.Wrap(err, "invalid subscription")
	}

	// Generate IDs
	subscription.ID = uuid.New().String()
	subscription.SubscriptionID = uuid.New().String()
	subscription.CreatedAt = time.Now().UTC()
	subscription.UpdatedAt = subscription.CreatedAt
	subscription.LastActive = subscription.CreatedAt

	// Store subscription
	if err := s.storeSubscription(ctx, subscription); err != nil {
		return errors.Wrap(err, "failed to store subscription")
	}

	// Cache subscription
	s.subsMutex.Lock()
	s.subscriptions[subscription.SubscriptionID] = subscription
	s.subsMutex.Unlock()

	s.logger.Info("Event subscription created",
		zap.String("subscription_id", subscription.SubscriptionID),
		zap.String("service", subscription.ServiceName),
		zap.String("topic_pattern", subscription.TopicPattern))

	return nil
}

// ConsumeEvents starts consuming events from Kafka topics
func (s *EventBusService) ConsumeEvents(ctx context.Context, topics []string) error {
	// Create consumer handler
	handler := &ConsumerGroupHandler{
		service: s,
		logger:  s.logger,
	}

	// Start consuming
	for {
		select {
		case <-ctx.Done():
			return ctx.Err()
		default:
			err := s.kafkaConsumer.Consume(ctx, topics, handler)
			if err != nil {
				s.kafkaOperations.WithLabelValues("consume", "error").Inc()
				s.logger.Error("Failed to consume events", zap.Error(err))
				time.Sleep(5 * time.Second) // Retry delay
				continue
			}
		}
	}
}

// ConsumerGroupHandler handles Kafka consumer group events
type ConsumerGroupHandler struct {
	service *EventBusService
	logger  *zap.Logger
}

func (h *ConsumerGroupHandler) Setup(sarama.ConsumerGroupSession) error {
	h.logger.Info("Kafka consumer group session started")
	return nil
}

func (h *ConsumerGroupHandler) Cleanup(sarama.ConsumerGroupSession) error {
	h.logger.Info("Kafka consumer group session ended")
	return nil
}

func (h *ConsumerGroupHandler) ConsumeClaim(session sarama.ConsumerGroupSession, claim sarama.ConsumerGroupClaim) error {
	for message := range claim.Messages() {
		h.service.processConsumedEvent(session.Context(), message)
		session.MarkMessage(message, "")
	}
	return nil
}

// processConsumedEvent processes an event consumed from Kafka
func (s *EventBusService) processConsumedEvent(ctx context.Context, message *sarama.ConsumerMessage) {
	start := time.Now()

	// Parse event from message
	var event models.Event
	if err := json.Unmarshal(message.Value, &event); err != nil {
		s.eventsConsumed.WithLabelValues(message.Topic, "unknown", "parse_error").Inc()
		s.logger.Error("Failed to parse consumed event", zap.Error(err))
		return
	}

	// Update event metadata
	event.Partition = int(message.Partition)
	event.Offset = message.Offset
	event.Status = "consumed"
	event.ProcessedAt = &[]time.Time{time.Now().UTC()}[0]

	// Update processing metrics
	eventType := "unknown"
	if val, exists := message.Headers[0]; exists && string(val.Key) == "event_type" {
		eventType = string(val.Value)
	}

	s.eventsConsumed.WithLabelValues(message.Topic, eventType, "success").Inc()
	s.eventProcessingTime.WithLabelValues("consume", eventType).Observe(time.Since(start).Seconds())

	// Process event based on type and route to appropriate handlers
	s.routeEvent(ctx, &event)

	s.logger.Debug("Event consumed and processed",
		zap.String("event_id", event.EventID),
		zap.String("topic", message.Topic),
		zap.Int64("offset", message.Offset))
}

// routeEvent routes events to appropriate handlers based on subscriptions
func (s *EventBusService) routeEvent(ctx context.Context, event *models.Event) {
	s.subsMutex.RLock()
	defer s.subsMutex.RUnlock()

	for _, subscription := range s.subscriptions {
		if subscription.Status != "active" {
			continue
		}

		// Check if event matches subscription criteria
		if s.eventMatchesSubscription(event, subscription) {
			// Send event to subscriber
			go s.deliverEventToSubscriber(ctx, event, subscription)
		}
	}
}

// eventMatchesSubscription checks if an event matches a subscription
func (s *EventBusService) eventMatchesSubscription(event *models.Event, subscription *models.EventSubscription) bool {
	// Check topic pattern (simplified - could use regex)
	if subscription.TopicPattern != "" && subscription.TopicPattern != "*" {
		if !strings.Contains(event.Topic, subscription.TopicPattern) {
			return false
		}
	}

	// Check event types
	if subscription.EventTypes != "" {
		var eventTypes []string
		if err := json.Unmarshal([]byte(subscription.EventTypes), &eventTypes); err == nil {
			found := false
			for _, et := range eventTypes {
				if et == event.EventType {
					found = true
					break
				}
			}
			if !found {
				return false
			}
		}
	}

	return true
}

// deliverEventToSubscriber delivers an event to a subscriber
func (s *EventBusService) deliverEventToSubscriber(ctx context.Context, event *models.Event, subscription *models.EventSubscription) {
	// For now, just log the delivery. In production, this would send HTTP requests,
	// publish to message queues, or trigger other services
	s.logger.Info("Event delivered to subscriber",
		zap.String("event_id", event.EventID),
		zap.String("subscriber", subscription.ServiceName),
		zap.String("method", subscription.Method))
}

// Helper methods

func (s *EventBusService) validateEvent(event *models.Event) error {
	if event.Topic == "" {
		return errors.New("topic is required")
	}
	if event.EventType == "" {
		return errors.New("event type is required")
	}
	if event.Source == "" {
		return errors.New("source is required")
	}
	return nil
}

func (s *EventBusService) validateTopic(topic *models.Topic) error {
	if topic.Name == "" {
		return errors.New("topic name is required")
	}
	if len(topic.Name) > 255 {
		return errors.New("topic name too long")
	}
	return nil
}

func (s *EventBusService) validateSubscription(subscription *models.EventSubscription) error {
	if subscription.ServiceName == "" {
		return errors.New("service name is required")
	}
	if subscription.Method == "" {
		return errors.New("method is required")
	}
	return nil
}

func (s *EventBusService) storeEvent(ctx context.Context, event *models.Event) error {
	payload := event.Payload
	if len(payload) > 10000 { // Truncate large payloads
		payload = payload[:10000] + "..."
	}

	_, err := s.db.Exec(ctx, `
		INSERT INTO events (id, event_id, event_type, topic, source, payload,
		                    partition, offset, status, timestamp, created_at, processed_at,
		                    retry_count, priority, is_replayable, is_encrypted)
		VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15, $16)
	`, event.ID, event.EventID, event.EventType, event.Topic, event.Source, payload,
		event.Partition, event.Offset, event.Status, event.Timestamp, event.CreatedAt, event.ProcessedAt,
		event.RetryCount, event.Priority, event.IsReplayable, event.IsEncrypted)

	return err
}

func (s *EventBusService) storeTopic(ctx context.Context, topic *models.Topic) error {
	_, err := s.db.Exec(ctx, `
		INSERT INTO topics (id, name, description, schema, partitions, replication_factor,
		                   retention_hours, status, category, owner_service, created_at, updated_at,
		                   is_system_topic, requires_auth, enable_dlq)
		VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15)
	`, topic.ID, topic.Name, topic.Description, topic.Schema, topic.Partitions, topic.ReplicationFactor,
		topic.RetentionHours, topic.Status, topic.Category, topic.OwnerService, topic.CreatedAt, topic.UpdatedAt,
		topic.IsSystemTopic, topic.RequiresAuth, topic.EnableDLQ)

	return err
}

func (s *EventBusService) storeSubscription(ctx context.Context, subscription *models.EventSubscription) error {
	_, err := s.db.Exec(ctx, `
		INSERT INTO event_subscriptions (id, subscription_id, service_name, topic_pattern, event_types,
		                               endpoint, filter, status, method, auth_token, max_retries, retry_delay,
		                               rate_limit_per_min, is_active, enable_batching, requires_ack,
		                               created_at, updated_at, last_active)
		VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15, $16, $17, $18, $19)
	`, subscription.ID, subscription.SubscriptionID, subscription.ServiceName, subscription.TopicPattern,
		subscription.EventTypes, subscription.Endpoint, subscription.Filter, subscription.Status,
		subscription.Method, subscription.AuthToken, subscription.MaxRetries, subscription.RetryDelay,
		subscription.RateLimitPerMin, subscription.IsActive, subscription.EnableBatching,
		subscription.RequiresAck, subscription.CreatedAt, subscription.UpdatedAt, subscription.LastActive)

	return err
}

// PERFORMANCE: Memory pool management functions
func getEvent() *models.Event {
	return eventPool.Get().(*models.Event)
}

func putEvent(event *models.Event) {
	// Reset fields for reuse
	event.ID = ""
	event.EventID = ""
	event.EventType = ""
	event.Topic = ""
	event.Source = ""
	event.Payload = ""
	event.Status = ""
	event.Priority = ""
	event.Partition = 0
	event.Offset = 0
	event.RetryCount = 0
	event.Timestamp = time.Time{}
	event.CreatedAt = time.Time{}
	event.ProcessedAt = nil
	event.IsReplayable = false
	event.IsEncrypted = false
	eventPool.Put(event)
}

func getGameEvent() *models.GameEvent {
	return gameEventPool.Get().(*models.GameEvent)
}

func putGameEvent(event *models.GameEvent) {
	// Reset fields for reuse
	event.EventID = ""
	event.EventType = ""
	event.Topic = ""
	event.Source = ""
	event.PlayerID = ""
	event.SessionID = ""
	event.GameMode = ""
	event.MapName = ""
	event.ActionType = ""
	event.ActionData = nil
	event.X = 0
	event.Y = 0
	event.Z = 0
	event.Timestamp = 0
	event.ServerID = ""
	event.Region = ""
	event.Version = ""
	gameEventPool.Put(event)
}

func getSystemEvent() *models.SystemEvent {
	return systemEventPool.Get().(*models.SystemEvent)
}

func putSystemEvent(event *models.SystemEvent) {
	// Reset fields for reuse
	event.EventID = ""
	event.EventType = ""
	event.Topic = ""
	event.Source = ""
	event.Component = ""
	event.InstanceID = ""
	event.Environment = ""
	event.EventLevel = ""
	event.EventData = nil
	event.CPUUsage = 0
	event.MemoryUsage = 0
	event.DiskUsage = 0
	event.TraceID = ""
	event.SpanID = ""
	systemEventPool.Put(event)
}

// Shutdown gracefully shuts down the event bus service
func (s *EventBusService) Shutdown(ctx context.Context) error {
	// Close event buffer
	close(s.eventBuffer)

	// Wait for workers to finish
	done := make(chan struct{})
	go func() {
		s.workersWG.Wait()
		close(done)
	}()

	select {
	case <-done:
		// Workers finished
	case <-time.After(30 * time.Second):
		s.logger.Warn("Workers did not finish within timeout")
	}

	// Close Kafka connections
	if s.kafkaProducer != nil {
		if err := s.kafkaProducer.Close(); err != nil {
			s.logger.Error("Error closing Kafka producer", zap.Error(err))
		}
	}

	if s.kafkaConsumer != nil {
		if err := s.kafkaConsumer.Close(); err != nil {
			s.logger.Error("Error closing Kafka consumer", zap.Error(err))
		}
	}

	if s.kafkaAdmin != nil {
		if err := s.kafkaAdmin.Close(); err != nil {
			s.logger.Error("Error closing Kafka admin", zap.Error(err))
		}
	}

	// Close database connection
	if s.db != nil {
		s.db.Close()
	}

	// Close Redis connection
	if s.redis != nil {
		if err := s.redis.Close(); err != nil {
			s.logger.Error("Error closing Redis", zap.Error(err))
		}
	}

	return nil
}

// ServeHTTP implements http.Handler interface
func (s *EventBusService) ServeHTTP(w http.ResponseWriter, r *http.Request) {
	// Route to appropriate handlers
	switch r.URL.Path {
	case "/metrics":
		promhttp.Handler().ServeHTTP(w, r)
	default:
		http.NotFound(w, r)
	}
}