// Code generated by NECPGAME backend agent. Enterprise-grade Telemetry Collector.
// Issue: #1495
// PERFORMANCE: Optimized async telemetry processing with Kafka integration

package telemetry

import (
	"context"
	"encoding/json"
	"sync"
	"time"

	"github.com/go-faster/errors"
	"github.com/google/uuid"
	"github.com/jackc/pgx/v5/pgxpool"
	"github.com/redis/go-redis/v9"
	"go.opentelemetry.io/otel/metric"
	"go.opentelemetry.io/otel/trace"
	"go.uber.org/zap"

	eventbus "github.com/your-org/necpgame/scripts/kafka-event-driven"
)

// Config holds telemetry collector configuration
type Config struct {
	DB       *pgxpool.Pool
	Redis    *redis.Client
	Logger   *zap.Logger
	Tracer   trace.Tracer
	Meter    metric.Meter
	EventBus *eventbus.EventBus
}

// TelemetryEvent represents a telemetry event
type TelemetryEvent struct {
	InstanceID      uuid.UUID `json:"instance_id"`
	AffixID         uuid.UUID `json:"affix_id"`
	EventType       string    `json:"event_type"`
	Data            map[string]interface{} `json:"data"`
	Timestamp       time.Time `json:"timestamp"`
}

// InstanceCompletion represents instance completion telemetry
type InstanceCompletion struct {
	InstanceID        uuid.UUID `json:"instance_id"`
	CompletionTime    int       `json:"completion_time_seconds"`
	Success           bool      `json:"success"`
	PlayerCount       int       `json:"player_count"`
	DeathsCount       int       `json:"deaths_count"`
	AffixIDs          []uuid.UUID `json:"affix_ids"`
	Timestamp         time.Time `json:"timestamp"`
}

// Collector handles affix telemetry collection and analysis
type Collector struct {
	db       *pgxpool.Pool
	redis    *redis.Client
	logger   *zap.Logger
	tracer   trace.Tracer
	meter    metric.Meter
	eventBus *eventbus.EventBus

	// Async processing
	eventChan chan *TelemetryEvent
	stopChan  chan struct{}
	wg        sync.WaitGroup

	// PERFORMANCE: Memory pooling for frequent allocations
	eventPool sync.Pool
}

// NewCollector creates a new telemetry collector
func NewCollector(cfg Config) (*Collector, error) {
	if cfg.DB == nil {
		return nil, errors.New("database connection is required")
	}
	if cfg.Logger == nil {
		return nil, errors.New("logger is required")
	}

	collector := &Collector{
		db:        cfg.DB,
		redis:     cfg.Redis,
		logger:    cfg.Logger,
		tracer:    cfg.Tracer,
		meter:     cfg.Meter,
		eventBus:  cfg.EventBus,
		eventChan: make(chan *TelemetryEvent, 1000), // Buffer for async processing
		stopChan:  make(chan struct{}),
	}

	// PERFORMANCE: Initialize memory pool for telemetry events
	collector.eventPool = sync.Pool{
		New: func() interface{} {
			return &TelemetryEvent{
				Data: make(map[string]interface{}, 8), // Pre-allocate capacity
			}
		},
	}

	return collector, nil
}

// CollectEvent collects a telemetry event
func (c *Collector) CollectEvent(ctx context.Context, event *TelemetryEvent) error {
	ctx, span := c.tracer.Start(ctx, "telemetry_collector.collect_event")
	defer span.End()

	// Store in database
	query := `
		INSERT INTO gameplay.affix_telemetry (
			id, instance_id, affix_id, completion_time, success, player_count, deaths_count, timestamp
		) VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
	`

	eventID := uuid.New()

	// Extract values from event data
	completionTime := 0
	if ct, ok := event.Data["completion_time"].(float64); ok {
		completionTime = int(ct)
	}

	success := false
	if s, ok := event.Data["success"].(bool); ok {
		success = s
	}

	playerCount := 1
	if pc, ok := event.Data["player_count"].(float64); ok {
		playerCount = int(pc)
	}

	deathsCount := 0
	if dc, ok := event.Data["deaths_count"].(float64); ok {
		deathsCount = int(dc)
	}

	_, err := c.db.Exec(ctx, query,
		eventID, event.InstanceID, event.AffixID,
		completionTime, success, playerCount, deathsCount, event.Timestamp)
	if err != nil {
		return errors.Wrap(err, "failed to insert telemetry event")
	}

	c.logger.Debug("Collected telemetry event",
		zap.String("event_type", event.EventType),
		zap.String("instance_id", event.InstanceID.String()),
		zap.String("affix_id", event.AffixID.String()))

	return nil
}

// CollectInstanceCompletion collects instance completion data
func (c *Collector) CollectInstanceCompletion(ctx context.Context, completion *InstanceCompletion) error {
	ctx, span := c.tracer.Start(ctx, "telemetry_collector.collect_instance_completion")
	defer span.End()

	// Store completion data for each affix
	for _, affixID := range completion.AffixIDs {
		event := &TelemetryEvent{
			InstanceID: completion.InstanceID,
			AffixID:    affixID,
			EventType:  "instance_completed",
			Data: map[string]interface{}{
				"completion_time": completion.CompletionTime,
				"success":         completion.Success,
				"player_count":    completion.PlayerCount,
				"deaths_count":    completion.DeathsCount,
			},
			Timestamp: completion.Timestamp,
		}

		if err := c.CollectEvent(ctx, event); err != nil {
			return errors.Wrap(err, "failed to collect completion event")
		}
	}

	c.logger.Info("Collected instance completion telemetry",
		zap.String("instance_id", completion.InstanceID.String()),
		zap.Bool("success", completion.Success),
		zap.Int("completion_time", completion.CompletionTime),
		zap.Int("affix_count", len(completion.AffixIDs)))

	return nil
}

// GetAffixPopularity returns popularity metrics for affixes
func (c *Collector) GetAffixPopularity(ctx context.Context, period string) (map[string]interface{}, error) {
	ctx, span := c.tracer.Start(ctx, "telemetry_collector.get_affix_popularity")
	defer span.End()

	// Calculate period start
	var periodStart time.Time
	now := time.Now()

	switch period {
	case "week":
		periodStart = now.AddDate(0, 0, -7)
	case "month":
		periodStart = now.AddDate(0, -1, 0)
	case "quarter":
		periodStart = now.AddDate(0, -3, 0)
	default:
		periodStart = now.AddDate(0, 0, -7) // default to week
	}

	query := `
		SELECT
			a.name,
			COUNT(at.id) as usage_count,
			AVG(at.completion_time) as avg_completion_time,
			SUM(CASE WHEN at.success THEN 1 ELSE 0 END)::float / COUNT(*) as success_rate
		FROM gameplay.affixes a
		LEFT JOIN gameplay.affix_telemetry at ON a.id = at.affix_id
		WHERE at.timestamp >= $1
		GROUP BY a.id, a.name
		ORDER BY usage_count DESC
	`

	rows, err := c.db.Query(ctx, query, periodStart)
	if err != nil {
		return nil, errors.Wrap(err, "failed to query popularity metrics")
	}
	defer rows.Close()

	result := make(map[string]interface{})
	var metrics []map[string]interface{}

	for rows.Next() {
		var name string
		var usageCount int
		var avgCompletionTime *float64
		var successRate *float64

		err := rows.Scan(&name, &usageCount, &avgCompletionTime, &successRate)
		if err != nil {
			return nil, errors.Wrap(err, "failed to scan popularity metrics")
		}

		metric := map[string]interface{}{
			"name":             name,
			"usage_count":      usageCount,
			"avg_completion_time": avgCompletionTime,
			"success_rate":     successRate,
		}
		metrics = append(metrics, metric)
	}

	result["period"] = period
	result["metrics"] = metrics

	return result, nil
}

// GetDifficultyMetrics returns difficulty analysis for affixes
func (c *Collector) GetDifficultyMetrics(ctx context.Context, period string) (map[string]interface{}, error) {
	ctx, span := c.tracer.Start(ctx, "telemetry_collector.get_difficulty_metrics")
	defer span.End()

	// Similar to popularity but focused on difficulty
	var periodStart time.Time
	now := time.Now()

	switch period {
	case "week":
		periodStart = now.AddDate(0, 0, -7)
	case "month":
		periodStart = now.AddDate(0, -1, 0)
	case "quarter":
		periodStart = now.AddDate(0, -3, 0)
	default:
		periodStart = now.AddDate(0, 0, -7)
	}

	query := `
		SELECT
			a.name,
			AVG(at.completion_time) as avg_completion_time,
			SUM(CASE WHEN at.success THEN 1 ELSE 0 END)::float / COUNT(*) as success_rate,
			AVG(at.deaths_count) as avg_deaths,
			COUNT(at.id) as sample_size
		FROM gameplay.affixes a
		JOIN gameplay.affix_telemetry at ON a.id = at.affix_id
		WHERE at.timestamp >= $1
		GROUP BY a.id, a.name
		HAVING COUNT(at.id) >= 10  -- Minimum sample size
		ORDER BY avg_completion_time DESC
	`

	rows, err := c.db.Query(ctx, query, periodStart)
	if err != nil {
		return nil, errors.Wrap(err, "failed to query difficulty metrics")
	}
	defer rows.Close()

	result := make(map[string]interface{})
	var metrics []map[string]interface{}

	for rows.Next() {
		var name string
		var avgCompletionTime, successRate, avgDeaths *float64
		var sampleSize int

		err := rows.Scan(&name, &avgCompletionTime, &successRate, &avgDeaths, &sampleSize)
		if err != nil {
			return nil, errors.Wrap(err, "failed to scan difficulty metrics")
		}

		// Calculate difficulty score (higher = more difficult)
		difficultyScore := 1.0
		if avgCompletionTime != nil && *avgCompletionTime > 0 {
			difficultyScore = *avgCompletionTime / 1800.0 // Normalize to 30-minute baseline
		}

		metric := map[string]interface{}{
			"name":               name,
			"avg_completion_time": avgCompletionTime,
			"success_rate":       successRate,
			"avg_deaths":         avgDeaths,
			"difficulty_score":   difficultyScore,
			"sample_size":        sampleSize,
		}
		metrics = append(metrics, metric)
	}

	result["period"] = period
	result["metrics"] = metrics

	return result, nil
}

// GetAffixPopularityForAPI returns popularity analytics for API
func (c *Collector) GetAffixPopularityForAPI(ctx context.Context, params interface{}) (*api.PopularityMetricsResponse, error) {
	ctx, span := c.tracer.Start(ctx, "telemetry_collector.get_affix_popularity_api")
	defer span.End()

	// Get raw popularity data
	rawData, err := c.GetAffixPopularity(ctx, "week") // Default to week
	if err != nil {
		return nil, errors.Wrap(err, "get popularity metrics")
	}

	// Convert to API format
	var metrics []api.PopularityMetricsResponseMetricsItem
	if rawMetrics, ok := rawData["metrics"].([]map[string]interface{}); ok {
		for _, raw := range rawMetrics {
			metric := api.PopularityMetricsResponseMetricsItem{
				Name:            raw["name"].(string),
				UsageCount:      int(raw["usage_count"].(float64)),
				PopularityRank:  int(raw["popularity_rank"].(float64)),
			}

			if winRate, ok := raw["win_rate"].(float64); ok {
				metric.WinRate = api.OptFloat32{Value: float32(winRate), Set: true}
			}

			metrics = append(metrics, metric)
		}
	}

	return &api.PopularityMetricsResponse{
		Period:  rawData["period"].(string),
		Metrics: metrics,
	}, nil
}

// GetAffixDifficulty returns difficulty analytics for API
func (c *Collector) GetAffixDifficulty(ctx context.Context, params interface{}) (*api.DifficultyMetricsResponse, error) {
	ctx, span := c.tracer.Start(ctx, "telemetry_collector.get_affix_difficulty")
	defer span.End()

	// Get raw difficulty data
	rawData, err := c.GetDifficultyMetrics(ctx, "week") // Default to week
	if err != nil {
		return nil, errors.Wrap(err, "get difficulty metrics")
	}

	// Convert to API format
	var metrics []api.DifficultyMetricsResponseMetricsItem
	if rawMetrics, ok := rawData["metrics"].([]map[string]interface{}); ok {
		for _, raw := range rawMetrics {
			metric := api.DifficultyMetricsResponseMetricsItem{
				AffixID:           uuid.MustParse(raw["affix_id"].(string)),
				Name:              raw["name"].(string),
				AvgCompletionTime: int(raw["avg_completion_time"].(float64)),
				DifficultyScore:   float32(raw["difficulty_score"].(float64)),
			}

			if failureRate, ok := raw["failure_rate"].(float64); ok {
				metric.FailureRate = api.OptFloat32{Value: float32(failureRate), Set: true}
			}

			metrics = append(metrics, metric)
		}
	}

	return &api.DifficultyMetricsResponse{
		Period:  rawData["period"].(string),
		Metrics: metrics,
	}, nil
}

// GetAffixCombinations returns combination analytics for API
func (c *Collector) GetAffixCombinations(ctx context.Context, params interface{}) (*api.CombinationAnalysisResponse, error) {
	ctx, span := c.tracer.Start(ctx, "telemetry_collector.get_affix_combinations")
	defer span.End()

	// Query for affix combinations
	query := `
		SELECT
			ARRAY_AGG(DISTINCT a.id) as affix_ids,
			ARRAY_AGG(DISTINCT a.name) as affix_names,
			COUNT(*) as usage_count,
			AVG(at.completion_time) as avg_completion_time,
			SUM(CASE WHEN at.success THEN 1 ELSE 0 END)::float / COUNT(*) as success_rate
		FROM gameplay.affix_telemetry at
		JOIN gameplay.affixes a ON at.affix_id = a.id
		WHERE at.timestamp >= NOW() - INTERVAL '7 days'
		GROUP BY at.instance_id
		HAVING COUNT(DISTINCT at.affix_id) > 1
		ORDER BY usage_count DESC, success_rate DESC
		LIMIT 20
	`

	rows, err := c.db.Query(ctx, query)
	if err != nil {
		return nil, errors.Wrap(err, "query affix combinations")
	}
	defer rows.Close()

	var combinations []api.CombinationAnalysisResponseCombinationsItem
	for rows.Next() {
		var affixIDs []uuid.UUID
		var affixNames []string
		var usageCount int
		var avgCompletionTime float64
		var successRate float64

		err := rows.Scan(&affixIDs, &affixNames, &usageCount, &avgCompletionTime, &successRate)
		if err != nil {
			c.logger.Error("Failed to scan combination row", zap.Error(err))
			continue
		}

		combination := api.CombinationAnalysisResponseCombinationsItem{
			AffixIds:          affixIDs,
			AffixNames:        affixNames,
			UsageCount:        usageCount,
			AvgCompletionTime: int(avgCompletionTime),
			SuccessRate:       float32(successRate),
		}

		combinations = append(combinations, combination)
	}

	if err := rows.Err(); err != nil {
		return nil, errors.Wrap(err, "iterate combination rows")
	}

	return &api.CombinationAnalysisResponse{
		Combinations: combinations,
	}, nil
}

// StartAsyncProcessing starts async telemetry processing (should run in background)
func (c *Collector) StartAsyncProcessing(ctx context.Context) {
	c.logger.Info("Starting async telemetry processing")

	// Start multiple worker goroutines for parallel processing
	workerCount := 3 // Configurable worker count
	for i := 0; i < workerCount; i++ {
		c.wg.Add(1)
		go c.telemetryWorker(ctx, i)
	}

	// Start batch processor for Redis queue
	c.wg.Add(1)
	go c.redisBatchProcessor(ctx)

	c.logger.Info("Async telemetry processing started", zap.Int("workers", workerCount))
}

// telemetryWorker processes telemetry events from the channel
func (c *Collector) telemetryWorker(ctx context.Context, workerID int) {
	defer c.wg.Done()

	c.logger.Debug("Started telemetry worker", zap.Int("worker_id", workerID))

	for {
		select {
		case <-ctx.Done():
			c.logger.Debug("Telemetry worker stopping due to context cancellation", zap.Int("worker_id", workerID))
			return
		case <-c.stopChan:
			c.logger.Debug("Telemetry worker stopping", zap.Int("worker_id", workerID))
			return
		case event := <-c.eventChan:
			if event == nil {
				continue
			}

			// Process telemetry event
			if err := c.processTelemetryEventAsync(ctx, event); err != nil {
				c.logger.Error("Failed to process telemetry event async",
					zap.Error(err),
					zap.String("instance_id", event.InstanceID.String()),
					zap.String("event_type", event.EventType))

				// Put back to Redis queue for retry
				if c.redis != nil {
					if err := c.queueEventToRedis(event); err != nil {
						c.logger.Error("Failed to queue event to Redis for retry", zap.Error(err))
					}
				}
			}

			// Return event to pool
			c.returnEventToPool(event)
		}
	}
}

// redisBatchProcessor processes events from Redis queue in batches
func (c *Collector) redisBatchProcessor(ctx context.Context) {
	defer c.wg.Done()

	if c.redis == nil {
		c.logger.Warn("Redis not available, Redis batch processor not started")
		return
	}

	c.logger.Debug("Started Redis batch processor")

	ticker := time.NewTicker(2 * time.Second) // Process every 2 seconds
	defer ticker.Stop()

	for {
		select {
		case <-ctx.Done():
			c.logger.Debug("Redis batch processor stopping due to context cancellation")
			return
		case <-c.stopChan:
			c.logger.Debug("Redis batch processor stopping")
			return
		case <-ticker.C:
			// Process batch from Redis queue
			if err := c.processRedisBatch(ctx); err != nil {
				c.logger.Error("Failed to process Redis batch", zap.Error(err))
			}
		}
	}
}

// processRedisBatch processes a batch of events from Redis queue
func (c *Collector) processRedisBatch(ctx context.Context) error {
	const batchSize = 10
	const queueKey = "telemetry:events"

	// Get events from Redis list (right pop)
	events, err := c.redis.LRange(ctx, queueKey, -batchSize, -1).Result()
	if err != nil && err != redis.Nil {
		return errors.Wrap(err, "failed to get events from Redis")
	}

	if len(events) == 0 {
		return nil // No events to process
	}

	processedCount := 0
	for _, eventJSON := range events {
		event := c.eventPool.Get().(*TelemetryEvent)

		// Parse event from JSON
		if err := json.Unmarshal([]byte(eventJSON), event); err != nil {
			c.logger.Error("Failed to unmarshal event from Redis", zap.Error(err))
			c.returnEventToPool(event)
			continue
		}

		// Process event
		if err := c.processTelemetryEventAsync(ctx, event); err != nil {
			c.logger.Error("Failed to process event from Redis batch", zap.Error(err))
		} else {
			processedCount++
		}

		c.returnEventToPool(event)
	}

	// Remove processed events from queue
	if processedCount > 0 {
		_, err = c.redis.LTrim(ctx, queueKey, 0, int64(len(events)-processedCount-1)).Result()
		if err != nil {
			c.logger.Error("Failed to trim Redis queue", zap.Error(err))
		}
	}

	if processedCount > 0 {
		c.logger.Debug("Processed Redis batch", zap.Int("events", processedCount))
	}

	return nil
}

// queueEventToRedis queues an event to Redis for later processing
func (c *Collector) queueEventToRedis(event *TelemetryEvent) error {
	if c.redis == nil {
		return errors.New("Redis not available")
	}

	eventJSON, err := json.Marshal(event)
	if err != nil {
		return errors.Wrap(err, "failed to marshal event")
	}

	// Push to Redis list (left push)
	return c.redis.LPush(context.Background(), "telemetry:events", eventJSON).Err()
}

// processTelemetryEventAsync processes a telemetry event asynchronously
func (c *Collector) processTelemetryEventAsync(ctx context.Context, event *TelemetryEvent) error {
	ctx, span := c.tracer.Start(ctx, "telemetry_collector.process_event_async")
	defer span.End()

	// PERFORMANCE: Add timeout for async processing
	ctx, cancel := context.WithTimeout(ctx, 10*time.Second)
	defer cancel()

	// Store in database
	query := `
		INSERT INTO gameplay.affix_telemetry (
			id, instance_id, affix_id, completion_time, success, player_count, deaths_count, timestamp
		) VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
	`

	eventID := uuid.New()

	// Extract values from event data
	completionTime := 0
	if ct, ok := event.Data["completion_time"].(float64); ok {
		completionTime = int(ct)
	}

	success := false
	if s, ok := event.Data["success"].(bool); ok {
		success = s
	}

	playerCount := 1
	if pc, ok := event.Data["player_count"].(float64); ok {
		playerCount = int(pc)
	}

	deathsCount := 0
	if dc, ok := event.Data["deaths_count"].(float64); ok {
		deathsCount = int(dc)
	}

	_, err := c.db.Exec(ctx, query,
		eventID, event.InstanceID, event.AffixID,
		completionTime, success, playerCount, deathsCount, event.Timestamp)
	if err != nil {
		return errors.Wrap(err, "failed to insert telemetry event")
	}

	// Publish event via EventBus if available
	if c.eventBus != nil {
		eventPayload := map[string]interface{}{
			"instance_id":     event.InstanceID.String(),
			"affix_id":        event.AffixID.String(),
			"event_type":      event.EventType,
			"completion_time": completionTime,
			"success":         success,
			"player_count":    playerCount,
			"deaths_count":    deathsCount,
			"processed_at":    time.Now(),
		}

		err = c.eventBus.PublishGameEvent(ctx, event.InstanceID.String(), eventbus.EventTypePlayerAction, eventPayload, "gameplay-service")
		if err != nil {
			c.logger.Error("Failed to publish telemetry event",
				zap.String("instance_id", event.InstanceID.String()),
				zap.Error(err))
			// Don't return error - telemetry storage succeeded
		}
	}

	c.logger.Debug("Processed telemetry event async",
		zap.String("instance_id", event.InstanceID.String()),
		zap.String("event_type", event.EventType))

	return nil
}

// CollectEventAsync collects a telemetry event asynchronously (non-blocking)
func (c *Collector) CollectEventAsync(event *TelemetryEvent) {
	select {
	case c.eventChan <- event:
		// Event queued successfully
	default:
		// Channel full, fallback to Redis queue
		if c.redis != nil {
			if err := c.queueEventToRedis(event); err != nil {
				c.logger.Error("Failed to queue telemetry event to Redis", zap.Error(err))
				// Return event to pool if queuing failed
				c.returnEventToPool(event)
			}
		} else {
			c.logger.Warn("Telemetry channel full and Redis not available, dropping event")
			c.returnEventToPool(event)
		}
	}
}

// Shutdown gracefully shuts down async processing
func (c *Collector) Shutdown(ctx context.Context) error {
	c.logger.Info("Shutting down telemetry collector")

	// Signal stop
	close(c.stopChan)

	// Wait for workers to finish
	done := make(chan struct{})
	go func() {
		c.wg.Wait()
		close(done)
	}()

	select {
	case <-done:
		c.logger.Info("Telemetry collector shut down gracefully")
		return nil
	case <-ctx.Done():
		c.logger.Warn("Telemetry collector shutdown timed out")
		return ctx.Err()
	}
}

// returnEventToPool returns a telemetry event to the memory pool
func (c *Collector) returnEventToPool(event *TelemetryEvent) {
	// Reset event fields
	event.InstanceID = uuid.Nil
	event.AffixID = uuid.Nil
	event.EventType = ""
	for k := range event.Data {
		delete(event.Data, k)
	}
	event.Timestamp = time.Time{}

	c.eventPool.Put(event)
}
